#!/usr/bin/env python3
"greedy-git - a tool for analysing remote git repos"
version = "0.1"

# https://github.com/strawp/greedy-git
# https://github.com/git/git/blob/master/Documentation/technical/index-format.txt

import binascii
import collections
import json
import mmap
import struct
import urllib.request
import re
import os.path
import operator
import zlib
import fnmatch
import sys

# Download the file at <url>/.git/index and then parse it
def get_index(baseurl):
  indexpath = url_to_domain( baseurl ) + "/index"
  if not os.path.isfile( indexpath ):
    url = baseurl + "/.git/index"
    try:
      urllib.request.urlretrieve( url, indexpath )
    except:
      print( "[E] Couldn't download index" )
      return False

    if not os.path.isfile( indexpath ):
      return False
  
  # Get json encoded version of index
  if not os.path.isfile( indexpath + ".json" ):
    j = parse_file( indexpath, False )
    f = open( indexpath + ".json", "w" )
    f.write( j )
    f.close()

  # Get human readable version of index
  if not os.path.isfile( indexpath + ".txt" ):
    txt = parse_file( indexpath, True )
    f = open( indexpath + ".txt", "w" )
    f.write( txt )
    f.close()

  # Produce flat text file list
  if not os.path.isfile( indexpath + ".lst" ):
    f = open( indexpath + ".json", "r" )
    index = json.load( f )
    f.close()
    flat = ""
    for item in index:
      flat += item.get('name','')+'\n'
    f = open( indexpath + ".lst", "w" )
    f.write( flat )
    f.close()

  # Get logs/HEAD
  get_head_log(baseurl)

  # Analyse the files
  write_report( url_to_domain( baseurl ) )
  return True

# Write out report.md based on files found 
def write_report( domain ):
 
  if os.path.isfile( domain + "/report.md" ):
    print( "[I] Already have report file" )
    return True

  print( "[I} Analysing index file list..." )
  rpt = "# Analysis of " + domain + " git repository\n"

  f = open( domain + "/index.lst", "r" )
  extensions = {}
  interesting = []
  uninteresting = "\.(jpg|png|jpeg|gif|css|js|html)$"
  for line in f:
    line = line.strip()
    if line == '':
      continue
    
    # Get extension
    m = re.search( "\.([a-z0-9]+)$", line, re.I )
    if m:
      ext = m.group(1)
      extensions[ext] = extensions.get(ext,0)+1

    # Get interesting
    m = re.search( "(setting|function|database|backup|config|pass|login|credential|^\.|\/\.|\.(sql|inc|ini|cfg|cnf|zip|gz|tar|7z|rar|bz|cpp|cs|xml|pl|cgi|vb)$)", line, re.I )
    if m:
      m2 = re.search( uninteresting, line, re.I )
      if m2: continue
      if line not in interesting:
        interesting.append(line)
  f.close()
  
  # Sum up file extensions
  rpt += "\n## Top file extensions\n"
  sorted_ext = sorted( extensions.items(), key=operator.itemgetter(1), reverse=True )
  for ext in sorted_ext:
    rpt += " * " + ext[0] + ": " + str(ext[1]) + '\n'
  
  # Strip list down a bit
  for name in interesting:
    m = re.search( uninteresting, name, re.I )
    if m:
      print( "removing uninteresting " + name )
      if not interesting.remove(name):
        print( "didn't remove " + name )

  # List interesting files
  rpt += "\n## Interesting files\n" 
  for name in interesting:
    rpt += " * " + name + '\n'

  f = open( domain + "/interesting.lst", "w" )
  f.write( '\n'.join(interesting) )
  f.close()
  print( "[I] Interesting files list written to " +  domain + "/interesting.lst" )

  # Look into logs/HEAD
  if os.path.isfile( domain + "/logs_HEAD" ):
    print( "[I] Analysing logs/HEAD" )

    emails = {}
    credentials = []
    f = open( domain + "/logs_HEAD" )
    for line in f:
      
      # Emails
      m = re.search( "<([^@]+)@([^>]+)>", line, re.I )
      if m: 
        email = m.group(1) + "@" + m.group(2)
        emails[email] = emails.get(email,0)+1

      # Credentials?
      m = re.search( ":\/\/([^:]+):([^@]+)@([^\s]+)", line )
      if m:
        credentials.append( m.group(0) )

    # Sort emails by frequency
    emails = sorted( emails.items(), key=operator.itemgetter(1), reverse=True )
    rpt += "\n## Top contributing users by log file activity\n"
    for email in emails:
      rpt += " * " + email[0] + ": " + str( email[1] ) + '\n' 

    rpt += "\n## Credentials in logs/HEAD\n"
    for cred in credentials:
      rpt += " * " + cred + '\n'
  else: print( "[E] " + domain + "/logs_HEAD doesn't exist" )

  f = open( domain + "/report.md", "w" )
  f.write( rpt )
  f.close()
  print( "[I] Overview report written to " + domain + "/report.md" )

# print report.md to stdout
def show_report(baseurl):
  print( "[I] Showing report:\n" );
  domain = url_to_domain(baseurl)
  if not os.path.isfile( domain + "/report.md" ):
    write_report( domain )
  f = open( domain + "/report.md" )
  print( f.read() )
  f.close()

# Attempt to download every known file
def get_all_files( baseurl ):
  print( "[W] Getting all known files at " + baseurl + ". This could take a while" )
  domain = url_to_domain(baseurl)
  f = open( domain + "/index.lst" )
  for line in f:
    line = line.strip()
    get_file_by_path( baseurl, line )
  print( "[I] Done." )

# Attempt to download every file in interesting.lst
def get_interesting_files(baseurl):
  print( "[I] Attempting to get interesting files from " + baseurl + "..." );
  domain = url_to_domain(baseurl)
  ipath = domain + "/interesting.lst"
  if not os.path.isfile( ipath ):
    write_report( domain )
  if not os.path.isfile( ipath ):
    print( "[W] No interesting files found to get\n" )
    return False
  f = open(ipath,"r")
  for line in f:
    line = line.strip()
    get_file_by_path(baseurl,line)

# Given a base url and a relative path, get the zlib object and unpack it
def get_file_by_path(baseurl,path):
  print( "[I] Getting " + path )
  domain = url_to_domain(baseurl)
  hashstr = get_hash_from_path(domain,path)
  # No exact match, attempt to use globbing on known files
  if not hashstr:
    hashes = get_hashes_by_glob( baseurl,path) 
    if len(hashes) == 0:
      print( "[E] No files matched " + path )
      return False
    for h in hashes.keys():
      print( "[I] Getting " + hashes[h] )
      get_file_by_hash(baseurl,h)
      unpack_object( domain, h, hashes[h] )
    return True

  # Get one file
  if not get_file_by_hash(baseurl,hashstr):
    return False
  return unpack_object( domain, hashstr, path )

# Get a list of matching hashes from a file glob
def get_hashes_by_glob( baseurl, glob ):
  domain = url_to_domain(baseurl)
  print( "[I] Getting hashes by file glob, " + glob )
  f = open( domain + "/index.json", "r" )
  index = json.load( f )
  f.close()
  hashes = {}
  for item in index:
    if fnmatch.fnmatch( item.get('name',''), glob ):
      hashes[item.get('sha1','')] = item.get('name','')
  return hashes

# Donwload a file given the sha1 hash 
def get_file_by_hash(baseurl,hashstr):
  print( "[I] Getting packed file " + hashstr )
  if not hashstr:
    return False
  if hashstr == '':
    return False
  domain = url_to_domain( baseurl )
  dest = domain + "/objects/" + hashstr[:2] + "/" + hashstr[2:]
  if os.path.isfile( dest ):
    print( "[I] " + hashstr + " already exists" )
    return True

  mkdir_p( os.path.dirname( dest ) )
  url = baseurl + "/.git/objects/" + hashstr[:2] + "/" + hashstr[2:]
  print( "[I] Attempting to download " + url )
  try:
    urllib.request.urlretrieve( url, dest )
  except:
    print( "[E] Failed to download" )
    return False

  if not os.path.isfile( dest ):
    print( "[E] Failed to download" )
    return False
  print( "[I] Size: " + str(os.path.getsize( dest )) )
  return True

# Attempt to unpack an object
def unpack_object( domain, hashstr, path ):
  zippath = domain + "/objects/" + hashstr[:2] + "/" + hashstr[2:] 
  dest = domain + "/files/" + path
  if os.path.isfile( dest ):
    print( "[I] " + dest + " already exists" )
    return True
  print( "[I] Attempting to unpack to " + dest )
  mkdir_p(os.path.dirname(dest))
  try:
    z = open( zippath, "rb" )
    f = open(dest,"wb")
    f.write(zlib.decompress(z.read()))
    f.close()
    z.close()
  except:
    print( "[E] Failed to decompress file: " + str( sys.exc_info() ))
    return False
    pass

  if os.path.isfile( dest ):
    print( "[I] Decompressed to " + str( os.path.getsize( dest ) ) + " bytes" )
    return True
  return False

# like "mkdir -p"
def mkdir_p(path):
  try:
    os.makedirs(path,0o777,True)
  except OSError as exc:
    if exc.errno == errno.EEXIST and os.path.isdir(path):
      pass
    else: raise

# Get a file hash given a definite path name
def get_hash_from_path(domain,path):
  f = open( domain + "/index.json", "r" )
  index = json.load( f )
  f.close()
  for item in index:
    if path == item.get('name','').strip():
      return item.get("sha1","")
  return False

# Attempt to download <url>/.git/config and also check it looks like a valid config file before saving
def get_config(baseurl):
  confpath = url_to_domain( baseurl ) + "/config"
  if os.path.isfile( confpath ):
    print( "[I] Already have config for " + baseurl )
    return True
  url = baseurl + "/.git/config"
  print( "[I] Attempting to download " + url ) 
  try:
    response = urllib.request.urlopen(url).read().decode("utf-8")
  except:
    print( "[E] Failed to download config file" )
    return False
  m = re.search( "repositoryformatversion = \d+", response, re.M )
  if not m:
    print( '[E] No valid looking config file found at ' + url )
    return False

  check_data_dir(url_to_domain( baseurl ) )
  f = open( confpath, "w" )
  f.write( response )
  f.close()
  return True

# Attempt to download <url>/.git/logs/HEAD
def get_head_log(baseurl):
  domain = url_to_domain( baseurl )
  url = baseurl + "/.git/logs/HEAD"
  headpath = domain + "/logs_HEAD"
  if os.path.isfile( headpath ): 
    print( "[I] Already got logs/HEAD" )
    return True

  print( "[I] Getting logs/HEAD" ) 
  try:
    response = urllib.request.urlopen(url).read().decode("utf-8")
    m = re.search( "[a-z0-9]{40} [a-z0-9]{40}", response, re.I )
    if not m: raise
    f = open( headpath, "w" )
    f.write( response )
    f.close()
  except:
    print( "[E] Failed to download logs/HEAD" )
    return False

  if os.path.isfile( headpath ): 
    print( "[I] Downloaded logs/HEAD" )
    return True
  
  print( "[E] Failed to download logs/HEAD" )
  return False

# Attempt to work out the id of the pack file for the project
def get_pack_id(baseurl):
  # If the web server has indexes enabled, requesting <url>/.git/objects/pack/ will reveal the file name
  if os.path.isfile( url_to_domain( baseurl ) + "/pack.id" ):
    print( "[I] Already have pack id for this repo" )
    return True

  url = baseurl + "/.git/objects/pack"
  try:
    response = urllib.request.urlopen(url).read().decode("utf-8")
  except:
    print( "[E] Failed to get pack dir listing" )
    response = None
    pass
  
  packid = None
  if response is not None:
    m = re.findall( "pack-([^\.]+)\.pack", response, re.M )
    if m:
      # If it got this far then it's worked!
      m = list_uniq( m )
      packid = '\n'.join(m)
      print( "[!] Holy crap, directory listing enabled in pack dir, got pack ids: " + ', '.join(m) );
    else:
      # Attempt to get objects/info/packs
      url = baseurl + "objects/info/packs"
      try:
        response = urllib.request.urlopen(url).read().decode("utf-8")
      except:
        print( "[E] Failed to get " + url )
        return False
      m = re.findall( "pack-([^\.]+)\.pack", response, re.M )
      if m:
        # If it got this far then it's worked!
        m = list_uniq( m )
        print( m )
        packid = '\n'.join(m)
        print( "[!] Found " + url + "! Pack ids: " + ', '.join(m) );

  if packid is None:
    return False
  f = open( url_to_domain(baseurl) + "/pack.id", "w" )
  f.write( packid )
  f.close()
  return True

# Check for info/refs and objects/info/packs to see if the repo is cloneable in standard git tools
def check_if_cloneable( baseurl ):
  print( "[I] Checking if this is cloneable" )
  domain = url_to_domain( baseurl )
  clonefile = domain + "/cloneable"
  if os.path.isfile( clonefile ):
    print("[!] domain " + clonefile + " already exists. Should be cloneable")
    return True
  url = baseurl + "/.git/info/refs"
  try:
    response = urllib.request.urlopen(url).read().decode("utf-8")
  except:
    print( "[E] Failed to get " + url + " - not cloneable" )
    return False

  m = re.search( "refs\/heads\/master", response )
  if not m:
    print( "[E] File at " + url + " doesn't look like a git refs file" )
    return False

  # Now look for objects/info/packs
  url = baseurl + "/.git/objects/info/packs" 
  try:
    response = urllib.request.urlopen(url).read().decode("utf-8")
  except:
    print( "[E] Failed to get " + url + " - not cloneable" )
    return False

  m = re.search( "pack-([^\.]+)\.pack", response, re.M )
  if not m:
    print( "[E] File at " + url + " doesn't look like a git packs file" )
    return False

  print( '[!] This looks like a cloneable repository! Try "git clone ' + baseurl + '/.git"')
  f = open( clonefile, "w" )
  f.write( "git clone " + baseurl + "/.git" )
  f.close()
  return True

# Check / make the data dir
def check_data_dir(dirname):
  if os.path.isdir( dirname ):
    return True

  os.makedirs(dirname)
  return os.path.isdir( dirname )
  
# Return the domain part of a URL
def url_to_domain( url ):
  p = re.compile( '^([a-z]+):\/\/([^\/]+)' )
  m = p.match(url)
  if m:  
    return m.group(2)
  return False

def list_uniq(seq): 
   # order preserving
   checked = []
   for e in seq:
       if e not in checked:
           checked.append(e)
   return checked

def check(boolean, message):
    if not boolean:
        import sys
        print("error: " + message, file=sys.stderr)
        sys.exit(1)

def parse(filename, pretty=True):
    with open(filename, "rb") as o:
        f = mmap.mmap(o.fileno(), 0, prot=mmap.PROT_READ)

        def read(format):
            # "All binary numbers are in network byte order."
            # Hence "!" = network order, big endian
            format = "! " + format
            bytes = f.read(struct.calcsize(format))
            return struct.unpack(format, bytes)[0]

        index = collections.OrderedDict()

        # 4-byte signature, b"DIRC"
        index["signature"] = f.read(4).decode("ascii")
        check(index["signature"] == "DIRC", "Not a Git index file")

        # 4-byte version number
        index["version"] = read("I")
        check(index["version"] in {2, 3},
            "Unsupported version: %s" % index["version"])

        # 32-bit number of index entries, i.e. 4-byte
        index["entries"] = read("I")

        yield index

        for n in range(index["entries"]):
            entry = collections.OrderedDict()

            entry["entry"] = n + 1

            entry["ctime_seconds"] = read("I")
            entry["ctime_nanoseconds"] = read("I")
            if pretty:
                entry["ctime"] = entry["ctime_seconds"]
                entry["ctime"] += entry["ctime_nanoseconds"] / 1000000000
                del entry["ctime_seconds"]
                del entry["ctime_nanoseconds"]

            entry["mtime_seconds"] = read("I")
            entry["mtime_nanoseconds"] = read("I")
            if pretty:
                entry["mtime"] = entry["mtime_seconds"]
                entry["mtime"] += entry["mtime_nanoseconds"] / 1000000000
                del entry["mtime_seconds"]
                del entry["mtime_nanoseconds"]

            entry["dev"] = read("I")
            entry["ino"] = read("I")

            # 4-bit object type, 3-bit unused, 9-bit unix permission
            entry["mode"] = read("I")
            if pretty:
                entry["mode"] = "%06o" % entry["mode"]

            entry["uid"] = read("I")
            entry["gid"] = read("I")
            entry["size"] = read("I")

            entry["sha1"] = binascii.hexlify(f.read(20)).decode("ascii")
            entry["flags"] = read("H")

            # 1-bit assume-valid
            entry["assume-valid"] = bool(entry["flags"] & (0b10000000 << 8))
            # 1-bit extended, must be 0 in version 2
            entry["extended"] = bool(entry["flags"] & (0b01000000 << 8))
            # 2-bit stage (?)
            stage_one = bool(entry["flags"] & (0b00100000 << 8))
            stage_two = bool(entry["flags"] & (0b00010000 << 8))
            entry["stage"] = stage_one, stage_two
            # 12-bit name length, if the length is less than 0xFFF (else, 0xFFF)
            namelen = entry["flags"] & 0xFFF

            # 62 bytes so far
            entrylen = 62

            if entry["extended"] and (index["version"] == 3):
                entry["extra-flags"] = read("H")
                # 1-bit reserved
                entry["reserved"] = bool(entry["extra-flags"] & (0b10000000 << 8))
                # 1-bit skip-worktree
                entry["skip-worktree"] = bool(entry["extra-flags"] & (0b01000000 << 8))
                # 1-bit intent-to-add
                entry["intent-to-add"] = bool(entry["extra-flags"] & (0b00100000 << 8))
                # 13-bits unused
                # used = entry["extra-flags"] & (0b11100000 << 8)
                # check(not used, "Expected unused bits in extra-flags")
                entrylen += 2

            if namelen < 0xFFF:
                entry["name"] = f.read(namelen).decode("utf-8", "replace")
                entrylen += namelen
            else:
                # Do it the hard way
                name = []
                while True:
                    byte = f.read(1)
                    if byte == "\x00":
                        break
                    name.append(byte)
                entry["name"] = b"".join(name).decode("utf-8", "replace")
                entrylen += 1

            padlen = (8 - (entrylen % 8)) or 8
            nuls = f.read(padlen)
            check(set(nuls) == {0}, "padding contained non-NUL")

            yield entry

        indexlen = len(f)
        extnumber = 1

        while f.tell() < (indexlen - 20):
            extension = collections.OrderedDict()
            extension["extension"] = extnumber
            extension["signature"] = f.read(4).decode("ascii")
            extension["size"] = read("I")

            # Seems to exclude the above:
            # "src_offset += 8; src_offset += extsize;"
            extension["data"] = f.read(extension["size"])
            extension["data"] = extension["data"].decode("iso-8859-1")
            if pretty:
                extension["data"] = json.dumps(extension["data"])

            yield extension
            extnumber += 1

        checksum = collections.OrderedDict()
        checksum["checksum"] = True
        checksum["sha1"] = binascii.hexlify(f.read(20)).decode("ascii")
        yield checksum

        f.close()

def parse_file(arg, pretty=True):
    rtn = ""
    if pretty:
       properties = {
           "version": "[header]",
           "entry": "[entry]",
           "extension": "[extension]",
           "checksum": "[checksum]"
       }
    else:
        rtn += "["

    for item in parse(arg, pretty=pretty):
        if pretty:
            for key, value in properties.items():
                if key in item:
                    rtn += value + '\n'
                    break
            else:
                rtn += "[?]"

        if pretty:
            for key, value in item.items():
                rtn += " " + str(key) + "=" + str(value) + '\n'
        else:
            rtn += json.dumps(item) + '\n'

        last = "checksum" in item
        if not last:
            if pretty:
                rtn += '\n'
            else:
                rtn += ',\n'

    if not pretty:
        rtn += "]"
    
    return rtn

def main():
    import argparse
    import sys

    parser = argparse.ArgumentParser(description="Analyse a remote .git dir")
    #parser.add_argument("-j", "--json", action="store_true",
    #    help="output JSON")
    parser.add_argument("-v", "--version", action="store_true",help="show script version number")
    parser.add_argument("-g", "--get", help="Path from repository to get")
    parser.add_argument("-r", "--report", action="store_true", help="Show report on repository")
    parser.add_argument("-I", "--get-interesting", action="store_true", help="Download and unpack any interesting files found")
    parser.add_argument("-a", "--all", action="store_true", help="Get all known files")
    parser.add_argument("-c", "--cloneable", action="store_true", help="Get check to see if there is enough metadata to just clone the entire repository using git tools")
    parser.add_argument("url", help="Base URL of a site to analyse")
    args = parser.parse_args()

    if args.version:
        print("greedy-git " + version)
        sys.exit()

    if not args.url:
        parser.print_usage()
        sys.exit(2)

    if not get_config(args.url):
      sys.exit()

    #get the index file
    if not get_index(args.url):
      sys.exit()

    if args.cloneable and check_if_cloneable( args.url ):
      sys.exit()
    
    # Attempt to work out the pack file id
    get_pack_id(args.url)

    if args.get:
      get_file_by_path(args.url,args.get)

    if args.report:
      show_report(args.url)

    if args.get_interesting:
      get_interesting_files(args.url)
    
    if args.all:
      get_all_files(args.url)

if __name__ == "__main__":
    main()
